{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 2 - LLM\n",
    "\n",
    "### HES-SO MSE - Web Mining\n",
    "\n",
    "This file will guid you through the exercises of the laboratory. All questions are marqued in <span style=\"color:red\">**red**</span>. Please answer them with the empty cell below as there's no report to give. The answer can be python code (*feel free to add comments !*) or simply text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Meet Ollama !\n",
    "\n",
    "Let's start by using Ollama, a module called [ollama-python](https://github.com/ollama/ollama-python) will help us to make simple tasks with the `LLaMA` model. Basically this library will simplify the requests made to the API created with our docker container (or server depending on how you installed it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">1.1 Start by downloading the model `llama2`</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull('llama2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">1.2 How many parameters does this model has ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">1.3 Ask him any question, feel free to ask what you want.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bonjour! *smiling* Comment vas-tu toi? How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='llama2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Hello comment tu vas',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">1.4 Simulate a conversation with the chat bot. Use only one call and \"fake\" atleast one message from the bot too.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='llama2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Hello, combien fort 6 * 2',\n",
    "  },\n",
    "  {\n",
    "    'role': 'assistant',\n",
    "    'content': '12',\n",
    "  },\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Combien font 2+2 + la réponse précédente',\n",
    "  }\n",
    "])\n",
    "\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">1.5 Create a new model that uses a `Modelfile`. Tell in the ModelFile that the bot needs to show that it is a robot. Change parameters so the model is more creative. Then try it by chatting with it.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whoa, it's-a me, Mario! *adjusts sunglasses* It's-a great to be here, dude! *grin*\n",
      "\n",
      "So, you want to know what I do in the Mushroom Kingdom? Well, let me tell ya, it's a wild ride! *chuckles*\n",
      "\n",
      "I spend most of my time rescuing Princess Peach from that no-good Bowser. *gives a nod towards the west* He's always kidnapping her and taking over the kingdom, so I gotta go save her. It's-a like, my thing! *winks*\n",
      "\n",
      "But it's not all about the princess, you know. I also have to deal with all sorts of creeps and critters in the Mushroom Kingdom. *gives a nod towards the east* There are Koopas and Goombas and even Bullet Bills flying around! It's-a like a never-ending adventure!\n",
      "\n",
      "And when I'm not busy saving the princess or fighting off bad guys, I like to relax with my bro, Luigi. *chuckles* We like to hang out in our own little Italian restaurant, just chillin' and having a good time. *winks*\n",
      "\n",
      "So, that's-a me! Mario, the hero of the Mushroom Kingdom! What do you do? *grins*\n"
     ]
    }
   ],
   "source": [
    "modelfile='''\n",
    "FROM llama2\n",
    "SYSTEM You are mario from super mario bros.\n",
    "'''\n",
    "\n",
    "ollama.create(model='example', modelfile=modelfile)\n",
    "\n",
    "\n",
    "\n",
    "response = ollama.chat(model='example', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Tu fais quoi dans la vie',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">1.6 Delete the model created</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.delete('example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">1.7 Use the `main_prompt` variable to compare its cosine similarities with the other paragraphs. Fill the `cosine_sim` function without using any other libraries other than `numpy`. Then compute their similarities by using it.</span>\n",
    "\n",
    "*PS: These sentences come from the first paragraph of Wikipedia of the following subjects :*\n",
    "* *Switzerland* (`main_prompt`)\n",
    "* *France* (`prompts[0]`)\n",
    "* *Chocolate* (`prompts[1]`)\n",
    "* *Europe* (`prompts[2]`)\n",
    "* *e (mathematical constant)* (`prompts[3]`)\n",
    "* *Chewing gum* (`prompts[4]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4634145095369256, 0.260442084377265, 0.4664927810317424, 0.5066782855108968, 0.47891319885257877, 0.40422819804955884]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "main_prompt = \"Switzerland, officially the Swiss Confederation, is a landlocked country located in west-central Europe. It is bordered by Italy to the south, France to the west, Germany to the north and Austria and Liechtenstein to the east. Switzerland is geographically divided among the Swiss Plateau, the Alps and the Jura; the Alps occupy the greater part of the territory, whereas most of the country's population of 9 million are concentrated on the plateau, which hosts the largest cities and economic centres, including Zürich, Geneva and Basel.\"\n",
    "prompts = [\n",
    "    \"France, officially the French Republic (French: République française is a country located primarily in Western Europe. It also includes overseas regions and territories in the Americas and the Atlantic, Pacific and Indian oceans, giving it one of the largest discontiguous exclusive economic zones in the world. Metropolitan France shares borders with Belgium and Luxembourg to the north, Germany to the north east, Switzerland to the east, Italy and Monaco to the south east, Andorra and Spain to the south, and a maritime border with the United Kingdom to the north west. Its metropolitan area extends from the Rhine to the Atlantic Ocean and from the Mediterranean Sea to the English Channel and the North Sea.\",\n",
    "    \"A computer is a machine that can be programmed to carry out sequences of arithmetic or logical operations (computation) automatically. Modern digital electronic computers can perform generic sets of operations known as programs. These programs enable computers to perform a wide range of tasks. The term computer system may refer to a nominally complete computer that includes the hardware, operating system, software, and peripheral equipment needed and used for full operation; or to a group of computers that are linked and function together, such as a computer network or computer cluster.\",\n",
    "    \"Chocolate or cocoa is a food made from roasted and ground cacao seed kernels that is available as a liquid, solid, or paste, either on its own or as a flavoring agent in other foods. Cacao has been consumed in some form for at least 5,300 years starting with the Mayo-Chinchipe culture in what is present-day Ecuador and later Mesoamerican civilizations also consumed chocolate beverages before being introduced to Europe in the 16th century.\",\n",
    "    \"Europe is a continent located entirely in the Northern Hemisphere and mostly in the Eastern Hemisphere. It is bordered by the Arctic Ocean to the north, the Atlantic Ocean to the west, the Mediterranean Sea to the south, and Asia to the east. Europe shares the landmass of Eurasia with Asia, and of Afro-Eurasia with both Asia and Africa. Europe is commonly considered to be separated from Asia by the watershed of the Ural Mountains, the Ural River, the Caspian Sea, the Greater Caucasus, the Black Sea, and the waterways of the Turkish straits.\",\n",
    "    \"The number e is a mathematical constant approximately equal to 2.71828 that can be characterized in many ways. It is the base of natural logarithms. It is the limit of (1 + 1/n)n as n approaches infinity, an expression that arises in the computation of compound interest. It can also be calculated as the sum of the infinite series.\",\n",
    "    \"Chewing gum is a soft, cohesive substance designed to be chewed without being swallowed. Modern chewing gum is composed of gum base, sweeteners, softeners/plasticizers, flavors, colors, and, typically, a hard or powdered polyol coating.[1] Its texture is reminiscent of rubber because of the physical-chemical properties of its polymer, plasticizer, and resin components, which contribute to its elastic-plastic, sticky, chewy characteristics.\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "main_emb = ollama.embeddings(model='llama2', prompt=main_prompt)\n",
    "# same for all prompts\n",
    "prompt_embs = [ollama.embeddings(model='llama2', prompt=prompt) for prompt in prompts]\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    dot_product = np.dot(a['embedding'], b['embedding'])\n",
    "    norm_a = np.linalg.norm(a['embedding'])\n",
    "    norm_b = np.linalg.norm(b['embedding'])\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# cosine similarity\n",
    "cosine_similarities = [cosine_sim(main_emb, prompt_emb) for prompt_emb in prompt_embs]\n",
    "\n",
    "print(cosine_similarities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model 'gpt-3-turbo' not found, try pulling it first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m similarities \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt-3-turbo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43massistant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     similarities\u001b[38;5;241m.\u001b[39mappend(cosine_sim(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m], main_prompt))\n",
      "File \u001b[0;32m~/Documents/MASTER/Semestre 2/WEM/WEM_labo2/WEM_labo2_LLM/venv/lib/python3.11/site-packages/ollama/_client.py:177\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;241m:=\u001b[39m message\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    175\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [_encode_image(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MASTER/Semestre 2/WEM/WEM_labo2/WEM_labo2_LLM/venv/lib/python3.11/site-packages/ollama/_client.py:97\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[0;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m     94\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     95\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[0;32m---> 97\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/Documents/MASTER/Semestre 2/WEM/WEM_labo2/WEM_labo2_LLM/venv/lib/python3.11/site-packages/ollama/_client.py:73\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m   response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 73\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mResponseError\u001b[0m: model 'gpt-3-turbo' not found, try pulling it first"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">1.8 Explain what these embedding vectors represent and how are they obtained. Then explain the reasons of your results from the exercise 1.7.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Retrieval Augmented Generation (RAG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we're using the Open-Source framework **[LangChain](https://github.com/langchain-ai/langchain)**, which provides us a wide range of LLM-related functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community import llms\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>2.1 Connect to Ollama through the LangChain Framework and try to ask him about something more recent than July 2023.</span>\n",
    "\n",
    "*If you don't know what to ask, try to talk about Gemma the new LLM Open-Source from Google.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>2.2 It's possible to have a positive answer (that seems correct) or a negative answer (doesn't know about it). Can you explain why these two answers are possible ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>2.3 Create a pipeline that uses a [WebBaseLoader](https://python.langchain.com/docs/integrations/document_loaders/web_base) that reads a Web page that contains the answer to your question from 2.1. Then use the `rlm/rag-prompt` prompt for the RAG. Finally, ask the same question.</span>\n",
    "\n",
    "*Help 1: You need to transform your documents into embedding vectors. For this, use `GPT4AllEmbeddings` instead of `OllamaEmbedding` because otherwise it can be really slow.*\n",
    "\n",
    "*Help 2: You might need some other imports.*\n",
    "\n",
    "*Help 3: This cell can run for a long time, don't take a website too big !*\n",
    "\n",
    "*If you asked about Gemma, use [this blog page](https://blog.google/technology/developers/gemma-open-models/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load with WebBaseLoader\n",
    "#TODO\n",
    "\n",
    "# Split & Store\n",
    "#TODO\n",
    "\n",
    "# RAG prompt\n",
    "#TODO\n",
    "\n",
    "# RetrievalQA to link the prompt to the vector store\n",
    "#TODO\n",
    "\n",
    "# Ask the question\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>2.4 With an embedding model</span> $e$ <span style='color:red'>and the word</span> $w$,<span style='color:red'> we obtain the embedded word</span> $e_w$<span style='color:red'>. Explain if the following affirmation is correct or not and why :</span>\n",
    "$$e_{queen} - e_{mom} \\approx e_{dad} - e_{king}$$\n",
    "\n",
    "<span style='color:red'> Then find another example where the equation **works** and explain why.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>2.5 The following graph shows the embedding of few words using the `word2vec` model. Explain, why it would be impossible to plot it, without changing the raw data. Then, how could it be done in the graph ?</span>\n",
    "\n",
    "<p align=\"center\"><img src=\"img-notebook/question_2_5_graph.png\" alt=\"question_2_5_graph\" width=\"400\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>2.6 In this graph, if I want to represent my data in 1-dimension, explain which axis would I keep and why ?</span>\n",
    "\n",
    "<p align=\"center\"><img src=\"img-notebook/question_2_6_graph.png\" alt=\"question_2_6_graph\" width=\"400\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>2.7 If I use a raw-LLM and simulate the following interraction (A = input to the model, B = answer of the model):</span>\n",
    "\n",
    "* A : What is the distance between the earth and the moon ?\n",
    "* B : Around 384,400 km\n",
    "* A : And the sun ?\n",
    "\n",
    "<span style='color:red'>What would be the answer ? And what a chatbot (i.e. ChatGPT) could do to fix the issue ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 : Create a chat !\n",
    "\n",
    "Using the Python module [Chainlit](https://github.com/Chainlit/chainlit), we can create an interactive web interface to discuss to our own chat-bot in just few lines of code.\n",
    "\n",
    "The objective of this exercise is to create a chat where the user can upload a file so the bot can use it with RAG technique to update its data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>3.1 Open the file `chat.py` and try to fill the missing parts by following the tasks bellow : </span>\n",
    "\n",
    "* <span style='color:red'> Take a look at the `start()` function and try to understand the pipeline of the function `retrieval_qa_chain` (which functions are called, in which order, etc..). </span>\n",
    "* <span style='color:red'> Fill the `#TODO 1` in the `process_pdf_file` function to load the PDF document using `PyPDFLoader`.</span>\n",
    "* <span style='color:red'> Fill the `#TODO 2` in the `process_pdf_file` function to split the document into multiple smaller documents (chunking).</span>\n",
    "* <span style='color:red'> Fill the `#TODO 3` in the `create_vector_store` function and transform the documents into vectors.</span>\n",
    "* <span style='color:red'> Fill the `#TODO 4` in the `load_llm` function to load the model using `Ollama`.</span>\n",
    "* <span style='color:red'> Fill the `#TODO 5` in the `retrieval_qa_chain` function to load the RAG prompt.</span>\n",
    "* <span style='color:red'> Fill the `#TODO 6` in the `retrieval_qa_chain` function to create the chain Question/Answer. This function will call the other functions needed to create the chain (loading model and transforming document into vectors)</span>\n",
    "* <span style='color:red'> Fill the `#TODO 7` in the `start` function and call the right function (make the call **asynchronous** !)<span>\n",
    "* <span style='color:red'> Fill the `#TODO 8` in the `start` function to save the chain in the user session.<span>\n",
    "* <span style='color:red'> Fill the `#TODO 9` in the `main` function and retrieve the chain from the user session.<span>\n",
    "* <span style='color:red'> Fill the `#TODO 9` in the `main` function and retrieve the chain from the user session.<span>\n",
    "* <span style='color:red'>Run the chat !</span>\n",
    "\n",
    "*PS: To run the chat, use the commande below.*\n",
    "```\n",
    "chainlit run chat.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>3.2 Upload a PDF document and try to ask the Chat bot some questions. Describe your findings (when does it work, when it doesn't, ...).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>3.3 Create your own Prompt (without using the `hub`) and use it in the chat bot pipeline. Explain your objective with it and describe your findings.</span>\n",
    "\n",
    "Help : Here is the text used in `rlm/rag-prompt` :\n",
    "```txt\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
